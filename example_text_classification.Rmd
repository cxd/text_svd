---
title: "Example Text Classification"
author: "Chris Davey"
date: '04-06-2017'
output: html_notebook
---
This document continues the process started in the example clustering notebook. We reload the data and generate the search space for documents and assign labels and clusters so that it is possible to generate a classifier.

Note below loads the decomposed tf-idf scaled matrix $T_{tf-idf}$ 

$$
USV' = T_{tf-idf}
$$

When projecting into the search space for either $U$ or $V$ either matrix is multiplied by the root of the diagonal matrix $S$.

$$
U_s = US^{1/2}
$$
$$
U_v = VS^{1/2}
$$

The singular values in $S$ describe the scaling of each dimension in either $U$ or $V$ in relation to the others. If interpreting eigenvalue in $S$ as being the variance explained for each component, the scaling multiplies by the standard deviation.


```{r}
require(dplyr)
require(tidyr)
require(ggplot2)
source("preprocess_text.R")
source("prepare_svd_document_model.R")
source("svd_search.R")
source("classification_metrics.R")
outputBase <- "data/csexample/"  

data <- load_document_data(outputBase)
doc_mat <- load_document_term_mat(outputBase)
  unique_words <- load_unique_words(outputBase)
  searchA <- load_search_space(outputBase)
  
  searchU <- searchA$u
  searchS <- searchA$d
  # transposed relation of attributes to objects rows equal the attributes
  # keep in mind that this search space is the transposed matrix $V'$.
  # without transposing the rows in $V'$ form the orthonormal basis.
  searchV <- searchA$v

  # max diagonal of eigenvalues
  n <- length(searchS)
temp <- rep(0, n*n)
Ss <- matrix(temp, nrow=n, ncol=n)
diag(Ss) <- sqrt(searchS)
  

```

Let's use the percentage of variance explained to determine the number of components to use in dimensionality reduction.

```{r}
total <- sum(searchS^2)
percent <- searchS^2/total
cumulative <- cumsum(percent)
df <- data.frame(component=1:length(searchS), percent=percent, cumulative=cumulative, variance=searchS^2)
df$component <- as.factor(df$component)

minVariance <- 0.99

idx <- which(df$cumulative >= 0.99)
nComponents <- min(idx)

df2 <- df[1:nComponents,]

ggplot(df2) +
  geom_bar(aes(x=component, y=percent), stat="identity") +
  ggtitle(paste("99% of variance explained in ", nComponents, "components"))



```


```{r}

cnts <- count(data, group_by=factor(app_tag))
k <- nrow(cnts)
# generate a document projection
Us <- searchU%*%Ss

# Instead of using the search matrix project the document matrix into the searchU matrix
# then use the first 100 components of Z
M <- as.matrix(doc_mat)
Z <- searchU%*%t(M)
dim(Z)
Z <- Z[,1:nComponents]

length(unique_words)

terms <- c(unique_words, "undefined")
colnames(Us) <- terms

# dimension m documents x n terms
dim(Us)

## clustering based on application tags.
clust <- kmeans(Z, k)
length(clust$cluster)

dataSet <- data.frame(Z)
dataSet$rowId <- data$row[1:nrow(Z)]
dataSet$appTag <- factor(data$app_tag[1:nrow(Z)])
dataSet$cluster <- factor(clust$cluster)

```


Some tags only have 1 example, we cannot train on those tags, we need at least more than one example, additionally any set of small examples is not guaranteed to be useful.

This means we need to obtain more samples for those tags. However in order to proceed we will need to remove those tags that do not have enough examples, initially we will use a threshold of at least > 10 tags.

```{r, fig.height=12, fig.width=8}


cnts <- cnts[order(cnts$n),]

ggplot(cnts, aes(group_by)) +
  geom_bar(aes(weight=n)) +
  scale_x_discrete(limits=cnts$group_by) +
  coord_flip() +
  theme(axis.text.y=element_text(size=4))
```

An important issue is the size of each of the groups, when preparing a data set that is used in supervised (or semi-supervised) learning, it is important to ensure that there is a large enough representation for each group. 

We will go ahead and remove the under represented groups which we consider to be any tag with less than 10 examples.

```{r}
# clean the data

idx <- which(cnts$n <= 10)
remove_tags <- cnts[idx,]$group_by

idx <- which(dataSet$appTag %in% remove_tags)

temp <- dataSet

# clean the data set
dataSet <- dataSet[-idx,]
```

We should also inspect the data associated with the clusters.


We first check the distribution of items within each cluster, the number of selected clusters we defined as the same number of tags.

```{r}
cnts2 <- count(dataSet, group_by=as.factor(cluster))
cnts2 <- cnts2[order(-cnts2$n),]
cnts2

cnts2 <- cnts2[order(cnts2$n),]

ggplot(cnts2, aes(group_by)) +
  geom_bar(aes(weight=n)) +
  scale_x_discrete(limits=cnts2$group_by) +
  coord_flip() +
  theme(axis.text.y=element_text(size=4))

```

And create a second data set filtering out those clusters with a membership of only 1.

```{r}
# create a second data set that is based on the clusters rather than tags.
idx <- which(cnts2$n <= 10)
remove_tags <- cnts2[idx,]$group_by

idx <- which(temp$cluster %in% remove_tags)


# clean the data set
dataSet2 <- temp[-idx,]

```

Now we can subset the data.

```{r}


require(caret)
require(MASS)
require(MVN)


# we need to drop the unused levels
# as we have removed a set of tags that are under represented
dataSet$appTag <- droplevels(dataSet$appTag)

set.seed(101)
inTrain <- createDataPartition(y=dataSet$appTag, p=0.75,list=FALSE)
dsTrain <- dataSet[inTrain,]
dsTest <- dataSet[-inTrain,]


## the data set using the cluster for group partition.
inTrain2 <- createDataPartition(y=as.factor(dataSet2$cluster), p=0.75,list=FALSE)
dsTrain2 <- dataSet2[inTrain2,]
dsTest2 <- dataSet2[-inTrain2,]

```

And attempt to build a partial least squares linear discriminant analysis as the first model.

```{r}
require(caret)

dsTrain$appTag <- droplevels(dsTrain$appTag)


# 100 is the number of components selected
#model1 <- plsda(dsTrain[,1:nComponents], y=dsTrain$appTag)

## 
model1 <- lda(dsTrain[,1:nComponents], grouping=dsTrain$appTag)

## save the model for reuse
 saveRDS(model1, file=paste(outputBase, "model1_lda.RData", sep=""))
 


dsTest$appTag <- droplevels(dsTest$appTag)



test1 <- predict(model1, dsTest[,1:nComponents])

predictedTags <- test1$class

remove(model1)
gc()
```

```{r}
# compute classification metrics
metrics <- classification_metrics(predictedTags, dsTest$appTag)


# we now need to tabulate the predictions
results <- metrics$resultTable

# identify the correct classifications, which are the sum of the diagonal
correct <- sum(diag(results))
accuracy <- metrics$uniformAccuracy
nonUniformAccuracy <- metrics$nonUniformAvgAccuracy
randomAccuracy <- metrics$randomAccuracy

heatmap(results, cex.lab=0.5, keep.dendro=FALSE, Rowv=NA, Colv=NA)

paste("Overall Uniform accuracy for manually labelled tags", round(accuracy*100, 2), "%")

paste("Avg accuracy for nonuniform distribution for manually labelled tags", round(nonUniformAccuracy*100, 2), "%")

paste("Random accuracy for manually labelled tags", round(randomAccuracy*100, 2), "%")

```
The heatmap for the results indicates there is a large bias in the model towards

- PaymentsEnquire

- ClaimStatus

- CardConcession

Note also that as the diagonal that is visible is due to the clustering presented in the heatmap.

We should examine the statistics for each individual class

```{r, fig.width=4, fig.height=8}
uniform <- metrics$uniformStats
uniform$proportion <- uniform$cnt / sum(uniform$cnt)
uniform <- uniform[order(-uniform$cnt),] 

uniform

uniform <- uniform[order(uniform$cnt),] 

ggplot(uniform, aes(labels)) +
  geom_bar(aes(weight=precision), fill="blue") +
  geom_bar(aes(weight=proportion), fill="red") +
  scale_x_discrete(limits=uniform$labels) +
  coord_flip() + 
  theme(axis.text.y=element_text(size=4)) +
  ggtitle("Tag Precision vs Instance Count in Test Set")



ggplot(uniform, aes(labels)) +
  geom_bar(aes(weight=recall), fill="blue") +
  geom_bar(aes(weight=proportion), fill="red") +
  scale_x_discrete(limits=uniform$labels) +
  coord_flip() + 
  theme(axis.text.y=element_text(size=4)) +
  ggtitle("Tag Recall vs Instance Count in Test Set")

```

The above provides a fit of a linear discriminant analysis model to the groups that have been manually defined. These groups are the app_tag that label each example in the corpus. The measures for precision (accuracy per term) and recall per term are given in the above charts as well as the number of instances per term in the test set. 

The resulting model performed better than random on the given test set, and performed well on the terms that have good representation within the data set.

The data set however is non-uniform, the app tags or classes do not have a uniform representation within the corpus, this makes it difficult for the model to discriminate terms that are not well represented.


Additionally, the linear discriminant model learns a set of linear boundaries between groups, this is the first approach to experiment with, it also assumes common variance and normality within groups. However is reasonably robust under these assumptions. The LDA is a good choice for initial experimentation with classification to determin whether partitions may be learnt within the data set. If it performs well, it may not be necessary to experiment with more computationally demanding models. 

Also the model has been training on a high number of dimensions for each sample some 1400 or so attributes in the projected variable $Z$, it may be desirable to apply a training algorithm such as k-fold to test feature reduction in order to determine an optimal choice of features. The number of features we have selected are based on the percentage of variance explained, with a cutoff of 99% of variation.


We can extend this method by making use of the a kernel based method in order to estimate non-linear boundaries within the data. 

There are several techniques, the first approach below is to use quadratic disciminant analysis.

The second will investigate the ldfa library provides a kernel discriminant analysis in support of this approach.


Lastly since the application tags may not necessarily describe suitable boundaries it is worthwhile experimenting with the clustering partitions that are defined by the clustering algorithm such as kmeans. We can use the cluster as the partition group and determine if a simple LDA can separate between these groups.

```{r}
require(glmnet)

# we need to use reduced dimensions so choosing initially the first 100 components.
n <- nComponents

X <- dsTrain[,1:n]

idx <- which(dsTrain$appTag == unique_words)

model1b <- glmnet(x=X, y=as.factor(dsTrain$aappTag), family="multinomial", type.multinomial = "grouped")


## save the model for reuse
saveRDS(model1b, file=paste(outputBase, "model1_glmnet_multinomial.RData", sep=""))

test1b <- predict(model1b, newdata=dsTest[,1:n])

predictedTags <- test1

gc()


```

```{r}
# compute classification metrics
metrics <- classification_metrics(predictedTags, dsTest$appTag)


# we now need to tabulate the predictions
results <- metrics$resultTable

# identify the correct classifications, which are the sum of the diagonal
correct <- sum(diag(results))
accuracy <- metrics$uniformAccuracy
nonUniformAccuracy <- metrics$nonUniformAvgAccuracy
randomAccuracy <- metrics$randomAccuracy

heatmap(results, cex.lab=0.5, keep.dendro=FALSE, Rowv=NA, Colv=NA)

paste("Overall Uniform accuracy for manually labelled tags", round(accuracy*100, 2), "%")

paste("Avg accuracy for nonuniform distribution for manually labelled tags", round(nonUniformAccuracy*100, 2), "%")

paste("Random accuracy for manually labelled tags", round(randomAccuracy*100, 2), "%")

```



